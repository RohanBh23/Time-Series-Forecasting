XGBoost, which stands for Extreme Gradient Boosting, is a scalable, distributed gradient-boosted decision tree (GBDT) machine learning library. It provides parallel tree boosting and is the leading machine learning library for regression, classification, and ranking problems.

XGBoost gained significant favor in the last few years as a result of helping individuals and teams win virtually every Kaggle structured data competition. In these competitions, companies and researchers post data after which statisticians and data miners compete to produce the best models for predicting and describing the data.
For complexity and high dimension data, XGBoost performs works better than Adaboost because XGBoost have system optimizations.

XGBoost incorporates additional regularization terms in its objective function to control the complexity of the model, thereby helping to avoid overfitting. The objective function in XGBoost consists of two main parts: the training loss and the regularization term.

1. **Training Loss (L(training))**: The training loss measures how well the model fits the training data. It quantifies the discrepancy between the actual target values and the predicted values generated by the model. XGBoost supports various loss functions depending on the type of problem being solved (e.g., regression, classification). For regression tasks, commonly used loss functions include mean squared error (MSE), while for classification tasks, it could be logistic loss or softmax loss.

2. **Regularization Term (Ω(f))**: The regularization term controls the complexity of the individual trees in the ensemble, preventing them from becoming overly complex and fitting noise in the data. This term helps to generalize the model better to unseen data and avoid overfitting.

   The regularization term in XGBoost can be written as the sum of two parts:

   a. **Tree Complexity Regularization**: This part of the regularization term penalizes the complexity of individual trees in the ensemble. It is calculated as the sum of the L1 (Lasso) and L2 (Ridge) regularization terms applied to the leaf weights of each tree.
   
   The L1 regularization term penalizes the sum of the absolute values of the leaf weights, while the L2 regularization term penalizes the sum of the squares of the leaf weights. This helps in simplifying the trees by encouraging smaller leaf weights, effectively pruning the trees and reducing their complexity.
   
   Mathematically, the tree complexity regularization term (Ω(f)) can be written as:
   
   $$Ω(f) = γ * T + ½ * λ * Σ(w²)$$

   Where:
   - T is the number of leaves in the tree.
   - γ is the regularization parameter controlling the impact of the number of leaves.
   - λ is the regularization parameter controlling the impact of the leaf weights.
   - w is the weight of each leaf node.

   b. **Regularization on the Number of Trees**: In addition to penalizing the complexity of individual trees, XGBoost also includes a term that penalizes the number of trees (K) in the ensemble. This term helps control the overall complexity of the model and prevent it from becoming overly complex.
   
   Mathematically, the regularization term on the number of trees can be written as:
   
   $$Ω(K) = η * K$$

   Where:
   - η is the regularization parameter controlling the impact of the number of trees.

The overall objective function in XGBoost is the sum of the training loss and the regularization term:

$$Objective = L(training) + Ω(f) + Ω(K)$$

By optimizing this objective function during training, XGBoost finds the optimal ensemble of trees that balances between minimizing the training loss and controlling the complexity of the model, thus avoiding overfitting. Adjusting the regularization parameters (γ, λ, and η) allows fine-tuning the model's complexity to achieve better generalization performance on unseen data.
